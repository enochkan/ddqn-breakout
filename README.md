# ddqn_breakout

In this study, we constructed a dueling DQN agent that was able to play the OpenAI Atari Breakout-v0.  We trained the agent for a total of 15000 episodes over 24 hours, and our trained agent was ableto achieve a high score of 24 during testing.

Due to time constraints, we were only able to run our agent for a total of 15000 epochs.  The original Atari Breakout experiment done by Mnih et.  al. ran a total of 100 epochs for training, where each training  epoch  contains  50000  minibatch  updates.   This  adds  up  to  around  5  million  minibatch updates, or 50 hours of clock time.  We were only able to perform around 60000 minibatch updates under 24 hours, assuming an average of 4 minibatch updates per epoch.  Training is done on a single RTX 2080 Ti GPU without parallelization.  Testing was done in a linux environment and was runon the same GPU. At each testing iteration, we record the score achieved by the bot as well as the high score from all the iterations.  Our bot was able to achieve a high score of 24 over the course of 80 epochs.  While this record is far from the current best record of 700+, we considered our training to be a success since there was clear evidence that the bot was able to learn.  Given more time to run our dDQN bot, we expect the testing performance of our bot to continually increase.

![dueling Deep Q Network Architecture](./assets/dueling-q-network.png?raw=true "dueling Deep Q Network Architecture")